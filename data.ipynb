{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Process data\n",
    "\n",
    "The objective is to learn about simple data curation practices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This notebook uses the [notMNIST](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html) dataset to be used with python experiments. This dataset is designed to look like the classic [MNIST](http://yann.lecun.com/exdb/mnist/) dataset, while looking a little more like real data: it's a harder task, and the data is a lot less 'clean' than MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "from IPython.display import display, Image\n",
    "from scipy import ndimage\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "# Config the matplotlib backend as plotting inline in IPython\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the data to the local machine\n",
    "- The data consists of characters rendered in a variety of fonts on a 28x28 image. \n",
    "- The labels are limited to 'A' through 'J' (10 classes). \n",
    "- The training set has about 500k and the testset 19000 labeled examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to download: notMNIST_large.tar.gz\n",
      "0%....5%....10%....15%....20%....25%....30%....35%....40%....45%....50%....55%....60%....65%....70%....75%....80%....85%....90%....95%....100%\n",
      "Download Complete!\n",
      "Found and verified ./data/notMNIST_large.tar.gz\n",
      "Attempting to download: notMNIST_small.tar.gz\n",
      "0%....5%....10%....15%....20%....25%....30%....35%....40%....45%....50%....55%....60%....65%....70%....75%....80%....85%....90%....95%....100%\n",
      "Download Complete!\n",
      "Found and verified ./data/notMNIST_small.tar.gz\n"
     ]
    }
   ],
   "source": [
    "url = 'https://commondatastorage.googleapis.com/books1000/'\n",
    "last_percent_reported = None\n",
    "data_root = './data/' # Change me to store data elsewhere\n",
    "\n",
    "def download_progress_hook(count, blockSize, totalSize):\n",
    "  \"\"\"A hook to report the progress of a download. This is mostly intended for users with\n",
    "  slow internet connections. Reports every 5% change in download progress.\n",
    "  \"\"\"\n",
    "  global last_percent_reported\n",
    "  percent = int(count * blockSize * 100 / totalSize)\n",
    "\n",
    "  if last_percent_reported != percent:\n",
    "    if percent % 5 == 0:\n",
    "      sys.stdout.write(\"%s%%\" % percent)\n",
    "      sys.stdout.flush()\n",
    "    else:\n",
    "      sys.stdout.write(\".\")\n",
    "      sys.stdout.flush()\n",
    "      \n",
    "    last_percent_reported = percent\n",
    "        \n",
    "def maybe_download(filename, expected_bytes, force=False):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  dest_filename = os.path.join(data_root, filename)\n",
    "  if force or not os.path.exists(dest_filename):\n",
    "    print('Attempting to download:', filename) \n",
    "    filename, _ = urlretrieve(url + filename, dest_filename, reporthook=download_progress_hook)\n",
    "    print('\\nDownload Complete!')\n",
    "  statinfo = os.stat(dest_filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified', dest_filename)\n",
    "  else:\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + dest_filename + '. Can you get to it with a browser?')\n",
    "  return dest_filename\n",
    "\n",
    "train_filename = maybe_download('notMNIST_large.tar.gz', 247336696)\n",
    "test_filename = maybe_download('notMNIST_small.tar.gz', 8458043)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the dataset \n",
    "Extract the dataset from the compressed .tar.gz file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data for ./data/notMNIST_large. This may take a while. Please wait.\n",
      "['./data/notMNIST_large/A', './data/notMNIST_large/B', './data/notMNIST_large/C', './data/notMNIST_large/D', './data/notMNIST_large/E', './data/notMNIST_large/F', './data/notMNIST_large/G', './data/notMNIST_large/H', './data/notMNIST_large/I', './data/notMNIST_large/J']\n",
      "Extracting data for ./data/notMNIST_small. This may take a while. Please wait.\n",
      "['./data/notMNIST_small/A', './data/notMNIST_small/B', './data/notMNIST_small/C', './data/notMNIST_small/D', './data/notMNIST_small/E', './data/notMNIST_small/F', './data/notMNIST_small/G', './data/notMNIST_small/H', './data/notMNIST_small/I', './data/notMNIST_small/J']\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "np.random.seed(133)\n",
    "\n",
    "def maybe_extract(filename, force=False):\n",
    "  root = os.path.splitext(os.path.splitext(filename)[0])[0]  # remove .tar.gz\n",
    "  if os.path.isdir(root) and not force:\n",
    "    # You may override by setting force=True.\n",
    "    print('%s already present - Skipping extraction of %s.' % (root, filename))\n",
    "  else:\n",
    "    print('Extracting data for %s. This may take a while. Please wait.' % root)\n",
    "    tar = tarfile.open(filename)\n",
    "    sys.stdout.flush()\n",
    "    tar.extractall(data_root)\n",
    "    tar.close()\n",
    "  data_folders = [\n",
    "    os.path.join(root, d) for d in sorted(os.listdir(root))\n",
    "    if os.path.isdir(os.path.join(root, d))]\n",
    "  if len(data_folders) != num_classes:\n",
    "    raise Exception(\n",
    "      'Expected %d folders, one per class. Found %d instead.' % (\n",
    "        num_classes, len(data_folders)))\n",
    "  print(data_folders)\n",
    "  return data_folders\n",
    "  \n",
    "train_folders = maybe_extract(train_filename)\n",
    "test_folders = maybe_extract(test_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display a sample of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABZ0lEQVR4nMWSPWuUQRSFn7u7UUHW\nZAV7wSqoYGVtYaMEbWxsBBGx8w9YqAj+AS3sREhtaxUNWlgIC4looRaSJiBidoWI2fed+1i82Q9k\nsfV0M885c5mZE0DYXj5/7uSxIwH48/vH9bUPdQhAh1NPh6YTpcPVM3QCIlq3fljqMqFZ6trB7Q4B\n7ftWddGcSLOufNgBblgVtZ4eW6s58iZxfLNrQLaqwR4AB5cWsgWwe5q7FjVdu9ADgN7FV6ZavEdf\ntfiyC0REAN11i2qfgerIFTrRJKPDiiPVndYigGyTza0x2UaApRb7fmY0Xoyhs9C/4Fz9Txj/gs6F\nwyZ3dC782uSvH8oYa/pCLxDaXn10YvLZkxmx/J42AIPdZi+/PPv0dnSgwXesUrNMa/Jro+liwuFV\n92pnC1YaZ0qw+KTOmWaaxSxa3IKI9pV3lTMavflsml4DokXv8uP+t33D740HC2efb+28vsQfU/It\nPYCuJtMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "image/png": {
       "height": 200,
       "width": 200
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(filename='./data/notMNIST_large/A/a2F6b28udHRm.png', width=200, height=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert into arrays\n",
    "- Convert the entire dataset into a 3D array (image index, x, y)(**number of data x 28 x 28**) of floating point values.\n",
    "- Normalize the data to have approximately zero mean and standard deviation ~0.5.\n",
    "- Skip images that are not readable.\n",
    "- Load each class into a separate dataset, since the computer might not be able to fit all data in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling ./data/notMNIST_large/A.pickle.\n",
      "./data/notMNIST_large/A\n",
      "Could not read: ./data/notMNIST_large/A/RnJlaWdodERpc3BCb29rSXRhbGljLnR0Zg==.png : cannot identify image file './data/notMNIST_large/A/RnJlaWdodERpc3BCb29rSXRhbGljLnR0Zg==.png' - it's ok, skipping.\n",
      "Could not read: ./data/notMNIST_large/A/SG90IE11c3RhcmQgQlROIFBvc3Rlci50dGY=.png : cannot identify image file './data/notMNIST_large/A/SG90IE11c3RhcmQgQlROIFBvc3Rlci50dGY=.png' - it's ok, skipping.\n",
      "Could not read: ./data/notMNIST_large/A/Um9tYW5hIEJvbGQucGZi.png : cannot identify image file './data/notMNIST_large/A/Um9tYW5hIEJvbGQucGZi.png' - it's ok, skipping.\n",
      "Full dataset tensor: (52909, 28, 28)\n",
      "Mean: -0.12825\n",
      "Standard deviation: 0.443121\n",
      "Pickling ./data/notMNIST_large/B.pickle.\n",
      "./data/notMNIST_large/B\n",
      "Could not read: ./data/notMNIST_large/B/TmlraXNFRi1TZW1pQm9sZEl0YWxpYy5vdGY=.png : cannot identify image file './data/notMNIST_large/B/TmlraXNFRi1TZW1pQm9sZEl0YWxpYy5vdGY=.png' - it's ok, skipping.\n",
      "Full dataset tensor: (52911, 28, 28)\n",
      "Mean: -0.00756303\n",
      "Standard deviation: 0.454491\n",
      "Pickling ./data/notMNIST_large/C.pickle.\n",
      "./data/notMNIST_large/C\n",
      "Full dataset tensor: (52912, 28, 28)\n",
      "Mean: -0.142258\n",
      "Standard deviation: 0.439806\n",
      "Pickling ./data/notMNIST_large/D.pickle.\n",
      "./data/notMNIST_large/D\n",
      "Could not read: ./data/notMNIST_large/D/VHJhbnNpdCBCb2xkLnR0Zg==.png : cannot identify image file './data/notMNIST_large/D/VHJhbnNpdCBCb2xkLnR0Zg==.png' - it's ok, skipping.\n",
      "Full dataset tensor: (52911, 28, 28)\n",
      "Mean: -0.0573678\n",
      "Standard deviation: 0.455648\n",
      "Pickling ./data/notMNIST_large/E.pickle.\n",
      "./data/notMNIST_large/E\n",
      "Full dataset tensor: (52912, 28, 28)\n",
      "Mean: -0.069899\n",
      "Standard deviation: 0.452942\n",
      "Pickling ./data/notMNIST_large/F.pickle.\n",
      "./data/notMNIST_large/F\n",
      "Full dataset tensor: (52912, 28, 28)\n",
      "Mean: -0.125583\n",
      "Standard deviation: 0.44709\n",
      "Pickling ./data/notMNIST_large/G.pickle.\n",
      "./data/notMNIST_large/G\n",
      "Full dataset tensor: (52912, 28, 28)\n",
      "Mean: -0.0945814\n",
      "Standard deviation: 0.44624\n",
      "Pickling ./data/notMNIST_large/H.pickle.\n",
      "./data/notMNIST_large/H\n",
      "Full dataset tensor: (52912, 28, 28)\n",
      "Mean: -0.0685221\n",
      "Standard deviation: 0.454232\n",
      "Pickling ./data/notMNIST_large/I.pickle.\n",
      "./data/notMNIST_large/I\n",
      "Full dataset tensor: (52912, 28, 28)\n",
      "Mean: 0.0307862\n",
      "Standard deviation: 0.468899\n",
      "Pickling ./data/notMNIST_large/J.pickle.\n",
      "./data/notMNIST_large/J\n",
      "Full dataset tensor: (52911, 28, 28)\n",
      "Mean: -0.153358\n",
      "Standard deviation: 0.443656\n",
      "Pickling ./data/notMNIST_small/A.pickle.\n",
      "./data/notMNIST_small/A\n",
      "Could not read: ./data/notMNIST_small/A/RGVtb2NyYXRpY2FCb2xkT2xkc3R5bGUgQm9sZC50dGY=.png : cannot identify image file './data/notMNIST_small/A/RGVtb2NyYXRpY2FCb2xkT2xkc3R5bGUgQm9sZC50dGY=.png' - it's ok, skipping.\n",
      "Full dataset tensor: (1872, 28, 28)\n",
      "Mean: -0.132626\n",
      "Standard deviation: 0.445128\n",
      "Pickling ./data/notMNIST_small/B.pickle.\n",
      "./data/notMNIST_small/B\n",
      "Full dataset tensor: (1873, 28, 28)\n",
      "Mean: 0.00535609\n",
      "Standard deviation: 0.457115\n",
      "Pickling ./data/notMNIST_small/C.pickle.\n",
      "./data/notMNIST_small/C\n",
      "Full dataset tensor: (1873, 28, 28)\n",
      "Mean: -0.141521\n",
      "Standard deviation: 0.44269\n",
      "Pickling ./data/notMNIST_small/D.pickle.\n",
      "./data/notMNIST_small/D\n",
      "Full dataset tensor: (1873, 28, 28)\n",
      "Mean: -0.0492167\n",
      "Standard deviation: 0.459759\n",
      "Pickling ./data/notMNIST_small/E.pickle.\n",
      "./data/notMNIST_small/E\n",
      "Full dataset tensor: (1873, 28, 28)\n",
      "Mean: -0.0599148\n",
      "Standard deviation: 0.45735\n",
      "Pickling ./data/notMNIST_small/F.pickle.\n",
      "./data/notMNIST_small/F\n",
      "Could not read: ./data/notMNIST_small/F/Q3Jvc3NvdmVyIEJvbGRPYmxpcXVlLnR0Zg==.png : cannot identify image file './data/notMNIST_small/F/Q3Jvc3NvdmVyIEJvbGRPYmxpcXVlLnR0Zg==.png' - it's ok, skipping.\n",
      "Full dataset tensor: (1872, 28, 28)\n",
      "Mean: -0.118185\n",
      "Standard deviation: 0.452279\n",
      "Pickling ./data/notMNIST_small/G.pickle.\n",
      "./data/notMNIST_small/G\n",
      "Full dataset tensor: (1872, 28, 28)\n",
      "Mean: -0.0925503\n",
      "Standard deviation: 0.449006\n",
      "Pickling ./data/notMNIST_small/H.pickle.\n",
      "./data/notMNIST_small/H\n",
      "Full dataset tensor: (1872, 28, 28)\n",
      "Mean: -0.0586893\n",
      "Standard deviation: 0.458759\n",
      "Pickling ./data/notMNIST_small/I.pickle.\n",
      "./data/notMNIST_small/I\n",
      "Full dataset tensor: (1872, 28, 28)\n",
      "Mean: 0.0526451\n",
      "Standard deviation: 0.471894\n",
      "Pickling ./data/notMNIST_small/J.pickle.\n",
      "./data/notMNIST_small/J\n",
      "Full dataset tensor: (1872, 28, 28)\n",
      "Mean: -0.151689\n",
      "Standard deviation: 0.448014\n"
     ]
    }
   ],
   "source": [
    "image_size = 28  # Pixel width and height.\n",
    "pixel_depth = 255.0  # Number of levels per pixel.\n",
    "\n",
    "def load_letter(folder, min_num_images):\n",
    "  \"\"\"Load the data for a single letter label.\"\"\"\n",
    "  image_files = os.listdir(folder)\n",
    "  dataset = np.ndarray(shape=(len(image_files), image_size, image_size),\n",
    "                         dtype=np.float32)\n",
    "  print(folder)\n",
    "  num_images = 0\n",
    "  for image in image_files:\n",
    "    image_file = os.path.join(folder, image)\n",
    "    try:\n",
    "      image_data = (ndimage.imread(image_file).astype(float) - \n",
    "                    pixel_depth / 2) / pixel_depth\n",
    "      if image_data.shape != (image_size, image_size):\n",
    "        raise Exception('Unexpected image shape: %s' % str(image_data.shape))\n",
    "      dataset[num_images, :, :] = image_data\n",
    "      num_images = num_images + 1\n",
    "    except IOError as e:\n",
    "      print('Could not read:', image_file, ':', e, '- it\\'s ok, skipping.')\n",
    "    \n",
    "  dataset = dataset[0:num_images, :, :]\n",
    "  if num_images < min_num_images:\n",
    "    raise Exception('Many fewer images than expected: %d < %d' %\n",
    "                    (num_images, min_num_images))\n",
    "    \n",
    "  print('Full dataset tensor:', dataset.shape)\n",
    "  print('Mean:', np.mean(dataset))\n",
    "  print('Standard deviation:', np.std(dataset))\n",
    "  return dataset\n",
    "        \n",
    "def maybe_pickle(data_folders, min_num_images_per_class, force=False):\n",
    "  dataset_names = []\n",
    "  for folder in data_folders:\n",
    "    set_filename = folder + '.pickle'\n",
    "    dataset_names.append(set_filename)\n",
    "    if os.path.exists(set_filename) and not force:\n",
    "      # You may override by setting force=True.\n",
    "      print('%s already present - Skipping pickling.' % set_filename)\n",
    "    else:\n",
    "      print('Pickling %s.' % set_filename)\n",
    "      dataset = load_letter(folder, min_num_images_per_class)\n",
    "      try:\n",
    "        with open(set_filename, 'wb') as f:\n",
    "          pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)\n",
    "      except Exception as e:\n",
    "        print('Unable to save data to', set_filename, ':', e)\n",
    "  \n",
    "  return dataset_names\n",
    "\n",
    "train_datasets = maybe_pickle(train_folders, 45000)\n",
    "test_datasets = maybe_pickle(test_folders, 1800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displaying a sample from an array\n",
    "Use [matplotlib.pyplot](https://matplotlib.org/users/image_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The label of the class: ./data/notMNIST_large/A.pickle\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wPkA1UsMjKxgAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x108c51860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class_index = 0;\n",
    "sample_class_path = train_datasets[class_index];\n",
    "sample_index = 1;\n",
    "\n",
    "def display_a_sample(sample_class_path,sample_index):\n",
    "    print(\"The label of the class:\", sample_class_path)\n",
    "    try:\n",
    "        with open(sample_class_path, 'rb') as f:\n",
    "            letter_set = pickle.load(f)\n",
    "            a_letter = letter_set[sample_index,:,:];\n",
    "            imgplot = plt.imshow(a_letter)\n",
    "    except Exception as e:\n",
    "        print('Unable to process data from', sample_class_path, ':', e)\n",
    "\n",
    "display_a_sample(sample_class_path,sample_index)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify that the data is balanced\n",
    "We expect the data to be balanced across classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of training data across classes: [ 52909.  52911.  52912.  52911.  52912.  52912.  52912.  52912.  52912.\n",
      "  52911.]\n",
      "Distribution of test data across classes: [ 1872.  1873.  1873.  1873.  1873.  1872.  1872.  1872.  1872.  1872.]\n"
     ]
    }
   ],
   "source": [
    "# train_datasets, test_datasets\n",
    "def verify_balance(pickle_files):\n",
    "  num_classes = len(pickle_files)\n",
    "  num_data_per_class = np.zeros(num_classes);\n",
    "\n",
    "  for i, pickle_file in enumerate(pickle_files):       \n",
    "    try:\n",
    "      with open(pickle_file, 'rb') as f:\n",
    "        letter_set = pickle.load(f)\n",
    "        num_data_per_class[i] = letter_set.shape[0]        \n",
    "    except Exception as e:\n",
    "      print('Unable to process data from', pickle_file, ':', e)\n",
    "    \n",
    "  return num_data_per_class\n",
    "\n",
    "print(\"Distribution of training data across classes:\", verify_balance(train_datasets))\n",
    "print(\"Distribution of test data across classes:\", verify_balance(test_datasets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge and prune data, and create a validation datasets\n",
    "Merge and prune the training data as needed. The labels will be stored into a separate array of integers 0 through 9. Also create a validation dataset for hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: (200000, 28, 28) (200000,)\n",
      "Validation: (10000, 28, 28) (10000,)\n",
      "Testing: (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "def make_arrays(nb_rows, img_size):\n",
    "  if nb_rows:\n",
    "    dataset = np.ndarray((nb_rows, img_size, img_size), dtype=np.float32)\n",
    "    labels = np.ndarray(nb_rows, dtype=np.int32)\n",
    "  else:\n",
    "    dataset, labels = None, None\n",
    "  return dataset, labels\n",
    "\n",
    "def merge_datasets(pickle_files, train_size, valid_size=0):\n",
    "  num_classes = len(pickle_files)\n",
    "  valid_dataset, valid_labels = make_arrays(valid_size, image_size)\n",
    "  train_dataset, train_labels = make_arrays(train_size, image_size)\n",
    "  vsize_per_class = valid_size // num_classes\n",
    "  tsize_per_class = train_size // num_classes\n",
    "    \n",
    "  start_v, start_t = 0, 0\n",
    "  end_v, end_t = vsize_per_class, tsize_per_class\n",
    "  end_l = vsize_per_class+tsize_per_class\n",
    "  for label, pickle_file in enumerate(pickle_files):       \n",
    "    try:\n",
    "      with open(pickle_file, 'rb') as f:\n",
    "        letter_set = pickle.load(f)\n",
    "        # let's shuffle the letters to have random validation and training set\n",
    "        np.random.shuffle(letter_set)\n",
    "        if valid_dataset is not None:\n",
    "          valid_letter = letter_set[:vsize_per_class, :, :]\n",
    "          valid_dataset[start_v:end_v, :, :] = valid_letter\n",
    "          valid_labels[start_v:end_v] = label\n",
    "          start_v += vsize_per_class\n",
    "          end_v += vsize_per_class\n",
    "                    \n",
    "        train_letter = letter_set[vsize_per_class:end_l, :, :]\n",
    "        train_dataset[start_t:end_t, :, :] = train_letter\n",
    "        train_labels[start_t:end_t] = label\n",
    "        start_t += tsize_per_class\n",
    "        end_t += tsize_per_class\n",
    "    except Exception as e:\n",
    "      print('Unable to process data from', pickle_file, ':', e)\n",
    "      raise\n",
    "    \n",
    "  return valid_dataset, valid_labels, train_dataset, train_labels\n",
    "            \n",
    "            \n",
    "train_size = 200000\n",
    "valid_size = 10000\n",
    "test_size = 10000\n",
    "\n",
    "valid_dataset, valid_labels, train_dataset, train_labels = merge_datasets(\n",
    "  train_datasets, train_size, valid_size)\n",
    "_, _, test_dataset, test_labels = merge_datasets(test_datasets, test_size)\n",
    "\n",
    "print('Training:', train_dataset.shape, train_labels.shape)\n",
    "print('Validation:', valid_dataset.shape, valid_labels.shape)\n",
    "print('Testing:', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle the data\n",
    "It's important to have data well shuffled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def randomize(dataset, labels):\n",
    "  permutation = np.random.permutation(labels.shape[0])\n",
    "  shuffled_dataset = dataset[permutation,:,:]\n",
    "  shuffled_labels = labels[permutation]\n",
    "  return shuffled_dataset, shuffled_labels\n",
    "train_dataset, train_labels = randomize(train_dataset, train_labels)\n",
    "test_dataset, test_labels = randomize(test_dataset, test_labels)\n",
    "valid_dataset, valid_labels = randomize(valid_dataset, valid_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify the shuffled data\n",
    "Verify that the data is still good after shuffling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAEHNJREFUeJzt3W2MXOV5xvHr2hev8QvBL9g4BGGIEBIKrYk2BjUUUZFE\nhLSCfAiB9MWNaExLaEtEpCD6IVSNFJo2obRNUpniYNqEhAYopKIp4LQ1KQ2wUGJsSAOlRtjyC7Cu\nsWNj7+7c/bAHtIE9zxnPzM4Z+/n/pNXOnnvOzr1n9tozs88553FECEB++upuAEA9CD+QKcIPZIrw\nA5ki/ECmCD+QKcIPZIrwA5ki/ECmBrr5YLM8FLM1t5sPmQUPlj+NS07fm1z3HX3pIzwbStedrEqu\nvMfRqWq7pfRVbLNn9i8orR3atUfje/Y3tdHbCr/tCyXdLKlf0t9GxI2p+8/WXJ3tC9p5yKOTK56r\nikOwB44/obR21b3/llz3I3NeT9b3Nw4l64Pub6t+tKrabv2J53zIg8l13zvy8dLaTz9za7qxKVp+\n2W+7X9JXJX1Y0hmSLrd9RqvfD0B3tfOef6Wk5yPihYg4JOnbki7uTFsAZlo74T9R0ktTvt5aLPs5\ntlfbHrE9MqaDbTwcgE6a8f/2R8SaiBiOiOFBDc30wwFoUjvh3ybppClfv6tYBuAI0E74H5d0mu1T\nbM+SdJmk+zrTFoCZ1vJQX0SM275a0r9ocqhvbURs7lhnOWnzakrj23eU1j7/xU8m133sDzYk6398\nfPopnYhGsn4wxkprVUNadar6uarG8ef0zUrWU0OBZ41cllz3+D8tf/v8wvbm9+dtjfNHxP2S7m/n\newCoB4f3Apki/ECmCD+QKcIPZIrwA5ki/ECm3M0Ze471wuCU3t7igfRo7yuffF+y/pnP3pms//r8\nV0trVWPp/e7dfVNV7+du/FiyPv9P5pXW/MiP0w+eOB340cZDei1Gmzqfv3e3LoAZRfiBTBF+IFOE\nH8gU4QcyRfiBTDHUd5SrGsqLRsXz35hIlvsXL0rWR28vv8z0j1Z8N7nuWKQfu+rKwKnhuKphxD2N\nA8n6+V+8Nllf8tVHkvXkFZvbGOJ8dOIBhvoApBF+IFOEH8gU4QcyRfiBTBF+IFOEH8hUV6foRvfF\n+Hhb63swfQnqiVfKT9mVpAXXLy2tvfK9nyXXXdyfns69+rLh5T/7HKd/rpX/cWWyvrxiHN9DFbNT\nTZQfw9Duc9Ys9vxApgg/kCnCD2SK8AOZIvxApgg/kCnCD2SqrXF+21sk7ZU0IWk8IoY70RR6R4yV\nTyXdDL+4vbT28kT6tPPF6dP1KzWUPg4gZWzHnPYevEK3xvJTOnGQz69ExCsd+D4AuoiX/UCm2g1/\nSHrA9hO2V3eiIQDd0e7L/nMjYpvtJZIetP2TiNgw9Q7FH4XVkjRbM/s+CkDz2trzR8S24vMuSfdI\nWjnNfdZExHBEDA+q4mQHAF3Tcvhtz7U9/43bkj4kaVOnGgMws9p52b9U0j2evATxgKRvRcT3O9IV\ngBnXcvgj4gVJv9jBXtCLUteXl6SqeR8S5633NHdvPou6MNQHZIrwA5ki/ECmCD+QKcIPZIrwA5ni\n0t3AdKKpWa6PaOz5gUwRfiBThB/IFOEHMkX4gUwRfiBThB/IFOEHMkX4gUwRfiBThB/IFOEHMkX4\ngUwRfiBThB/IFOEHMkX4gUwRfiBThB/IFOEHMkX4gUwRfiBThB/IVGX4ba+1vcv2pinLFtp+0PZz\nxecFM9smgE5rZs9/m6QL37LsOknrI+I0SeuLrwEcQSrDHxEbJI2+ZfHFktYVt9dJuqTDfQGYYa2+\n518aEduL2zskLe1QPwC6pO1/+EVESIqyuu3Vtkdsj4zpYLsPB6BDWg3/TtvLJKn4vKvsjhGxJiKG\nI2J4UEMtPhyATms1/PdJWlXcXiXp3s60A6Bbmhnqu0PSf0o63fZW21dIulHSB20/J+kDxdcAjiAD\nVXeIiMtLShd0uBcAXcQRfkCmCD+QKcIPZIrwA5ki/ECmCD+QKcIPZIrwA5ki/ECmCD+QKcIPZIrw\nA5ki/ECmCD+QKcIPZIrwA5ki/ECmCD+QKcIPZIrwA5ki/ECmCD+QqcpLd/cUu7zU31+xbsXfuWi0\nvn7FujE+nv7eQA3Y8wOZIvxApgg/kCnCD2SK8AOZIvxApgg/kKnKcX7bayX9qqRdEfGeYtkNkj4l\n6eXibtdHxP1td9NXMVafGE9nLB04PM3s+W+TdOE0y2+KiBXFR/vBB9BVleGPiA2SRrvQC4Auauc9\n/9W2N9pea3tBxzoC0BWthv/rkt4taYWk7ZK+XHZH26ttj9geGdPBFh8OQKe1FP6I2BkRExHRkHSL\npJWJ+66JiOGIGB7UUKt9AuiwlsJve9mULz8qaVNn2gHQLc0M9d0h6XxJi21vlfR5SefbXiEpJG2R\ndOUM9ghgBlSGPyIun2bxrS0/YmosvzGRXLV/8aLS2rbfOD257sFf2pusHzfvQLK+c8dxpbUTHkpv\nxnfcOZKsc4wC6sARfkCmCD+QKcIPZIrwA5ki/ECmCD+Qqe5fujsxnHfwovclV/3dm75bWrts/vrk\nuhMVl9fur7i091iU9z14YfpU5LOOuypZX/K1R5J1D6SfJoYK0Qr2/ECmCD+QKcIPZIrwA5ki/ECm\nCD+QKcIPZKq74/zzjlHjrBWl5Rv/+m+Sq58zu3w8fX/jUMttSdIcz0rWz9t4aWlt4JbFyXWX/WBz\nsp4+kfkIH8dPTJ3e7+hiI4fHFTO2Hw3Y8wOZIvxApgg/kCnCD2SK8AOZIvxApgg/kKmujvOPn9DQ\n6Of2l9ZT4/iStK/xemlt0Ol1hzyYrJ/+8G8l68s/vrG86BeS605E745nzzQPlR8/0a+Z3S59bezb\n+sbcwU56E3t+IFOEH8gU4QcyRfiBTBF+IFOEH8gU4QcyVTnOb/skSbdLWiopJK2JiJttL5T0HUnL\nJW2RdGlE7E59r1OOeVV/f+ZtiXvMSfZyTOKc+6rr7led7//Otenz+VP6hoaS9cbr5ccnHO1iXvlz\nOtjDQ+muushClUbvH9vRzJ5/XNK1EXGGpHMkfdr2GZKuk7Q+Ik6TtL74GsARojL8EbE9Ip4sbu+V\n9KykEyVdLGldcbd1ki6ZqSYBdN5hvee3vVzSWZIelbQ0IrYXpR2afFsA4AjRdPhtz5N0l6RrIuK1\nqbWICGn6A7Vtr7Y9Yntk92gGF0YDjhBNhd/2oCaD/82IuLtYvNP2sqK+TNKu6daNiDURMRwRwwsW\nMrgA9IrKNNq2pFslPRsRX5lSuk/SquL2Kkn3dr49ADOlmVN63y/pNyU9bfupYtn1km6UdKftKyS9\nKKn82tZvPJilRf31DIHsi7Fkfdae1i/93TiU/t5HtIohVCWmLpekgycvLK0t7mt9eLUZ/W59LHFg\nfw+PQ3ZIZfgj4oeSyrbEBZ1tB0C38CYcyBThBzJF+IFMEX4gU4QfyBThBzLV1Ut37xqfq7989ZzS\n+heWPJ1cv5G41PN4xTj+kv65yfqLF6XrJ/+ovNY3K31Z8BhPjxnHRLvnj7bOiSm0JckD6V+Rxuvp\n3l+6oHwsf07FOP/Biud0QK1frr3qex//VHvTotf5nDaLPT+QKcIPZIrwA5ki/ECmCD+QKcIPZIrw\nA5nq6jj/7t3zdPddv1xa/8Lvpcf5xxLnjg+5vR/ln1f9WbL+iZ98trR27LcSBwFIUtV55TVO4V01\nHh3j6fHuvZeVH7chSfd+4suJavpS7VXj+AcifQ2GeZ5dWrt66/nJdWd/77FkvVKDcX4APYrwA5ki\n/ECmCD+QKcIPZIrwA5ki/ECmHF0cYz7WC+Psvg+U1g98f3ly/Q1n3lNa29dIT4M96NbP/ZakPY0D\npbX3P/Y7yXWPvXN+sr7gsR3JeowmZz5PX1t/0XHJVXevTE+x+LNL9yTrjwx/I1mf11c+1l51Tv1E\nxe9m1fUANh8qf86uWXVVct2+f/+vZF196d+nusb5H431ei1Gm5p0gD0/kCnCD2SK8AOZIvxApgg/\nkCnCD2SK8AOZqhznt32SpNslLZUUktZExM22b5D0KUkvF3e9PiLuT32vY70wznb5rN79i8rncpek\nHd84vrT25PB3kutORCNZT80JMFkvX7/qGIEqW8f3JevPHFqQrPe7vLczBtPj9MsG5iXrVarG6vsS\n+5e+0pnfJ/Wnjl+Q9Fe7T07W//H3y48pGfjBE8l1e/kaDCmHM87fzBUwxiVdGxFP2p4v6QnbDxa1\nmyLiz1ttFEB9KsMfEdslbS9u77X9rKQTZ7oxADPrsN7z214u6SxJjxaLrra90fZa29O+NrW92vaI\n7ZExHWyrWQCd03T4bc+TdJekayLiNUlfl/RuSSs0+cpg2ou1RcSaiBiOiOFBDXWgZQCd0FT4bQ9q\nMvjfjIi7JSkidkbEREQ0JN0iaeXMtQmg0yrDb9uSbpX0bER8ZcryZVPu9lFJmzrfHoCZ0sxQ37mS\nHpb0tPTmeNf1ki7X5Ev+kLRF0pXFPwdLVZ3SW91t+d+q0VXpFx5nX/Vksv6lZQ8n61Wnj+LwVQ3V\nfe0fPpKsn/IXm5P1if9LDHP26Cm57eroUF9E/FCadkA2OaYPoLdxhB+QKcIPZIrwA5ki/ECmCD+Q\nKcIPZKr7l+5OnNJbdRql+8vHZqumkvZgepx+7Lwzk/Utv1Z+2u6pZ25Lrvuxd6ZPH105+3+T9ZMH\n0mPOfYnttnMifSrzIwdOSdb/6eVfSNaf2HRqsn7iQ+W9zX/gmeS6jb17k/WqsXr3lT921e/LkYpL\ndwOoRPiBTBF+IFOEH8gU4QcyRfiBTBF+IFNdHee3/bKkF6csWizpla41cHh6tbde7Uuit1Z1sreT\nI6L8GvdTdDX8b3tweyQihmtrIKFXe+vVviR6a1VdvfGyH8gU4QcyVXf419T8+Cm92luv9iXRW6tq\n6a3W9/wA6lP3nh9ATWoJv+0Lbf+37edtX1dHD2Vsb7H9tO2nbI/U3Mta27tsb5qybKHtB20/V3xO\nT+Hb3d5usL2t2HZP2b6opt5Osv2vtp+xvdn2HxbLa912ib5q2W5df9lvu1/STyV9UNJWSY9Lujwi\n0id3d4ntLZKGI6L2MWHb50naJ+n2iHhPsexLkkYj4sbiD+eCiPhcj/R2g6R9dc/cXEwos2zqzNKS\nLpH026px2yX6ulQ1bLc69vwrJT0fES9ExCFJ35Z0cQ199LyI2CBp9C2LL5a0rri9TpO/PF1X0ltP\niIjtEfFkcXuvpDdmlq512yX6qkUd4T9R0ktTvt6q3pryOyQ9YPsJ26vrbmYaS6fMjLRD0tI6m5lG\n5czN3fSWmaV7Ztu1MuN1p/EPv7c7NyLeK+nDkj5dvLztSTH5nq2Xhmuamrm5W6aZWfpNdW67Vme8\n7rQ6wr9N0klTvn5XsawnRMS24vMuSfeo92Yf3vnGJKnF51019/OmXpq5ebqZpdUD266XZryuI/yP\nSzrN9im2Z0m6TNJ9NfTxNrbnFv+Ike25kj6k3pt9+D5Jq4rbqyTdW2MvP6dXZm4um1laNW+7npvx\nOiK6/iHpIk3+x/9/JP1RHT2U9HWqpB8XH5vr7k3SHZp8GTimyf+NXCFpkaT1kp6T9JCkhT3U299p\ncjbnjZoM2rKaejtXky/pN0p6qvi4qO5tl+irlu3GEX5ApviHH5Apwg9kivADmSL8QKYIP5Apwg9k\nivADmSL8QKb+H4TILe6AV9r7AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10ba7e0b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_index = 1;\n",
    "a_sample = train_dataset[sample_index,:,:];\n",
    "imgplot = plt.imshow(a_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the data\n",
    "Save the data for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressed pickle size: 690800506\n"
     ]
    }
   ],
   "source": [
    "pickle_file = os.path.join(data_root, 'notMNIST.pickle')\n",
    "\n",
    "try:\n",
    "  f = open(pickle_file, 'wb')\n",
    "  save = {\n",
    "    'train_dataset': train_dataset,\n",
    "    'train_labels': train_labels,\n",
    "    'valid_dataset': valid_dataset,\n",
    "    'valid_labels': valid_labels,\n",
    "    'test_dataset': test_dataset,\n",
    "    'test_labels': test_labels,\n",
    "    }\n",
    "  pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "  f.close()\n",
    "except Exception as e:\n",
    "  print('Unable to save data to', pickle_file, ':', e)\n",
    "  raise\n",
    "    \n",
    "statinfo = os.stat(pickle_file)\n",
    "print('Compressed pickle size:', statinfo.st_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train an logistic regression\n",
    "Train a logistic regression model on this data using 50, 100, 1000 and 5000 training samples using [sklearn.linear_model.LogistricRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression).\n",
    "### Reference\n",
    "[Notes of logistic regression](https://github.com/xiangli-chen/home/blob/master/machine-learning/general-supervised-learning.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Test set (10000, 28, 28) (10000,)\n",
      "Number of training data: 50 accuracy: 0.4849\n",
      "Number of training data: 100 accuracy: 0.7136\n",
      "Number of training data: 1000 accuracy: 0.8425\n",
      "Number of training data: 5000 accuracy: 0.8438\n",
      "Number of training data: 200000 accuracy: 0.8999\n"
     ]
    }
   ],
   "source": [
    "# reformulate the data\n",
    "# data: {array-like, sparse matrix}, shape (n_samples, n_features)\n",
    "# label: rray-like, shape (n_samples,)\n",
    "\n",
    "pickle_file = './data/notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)\n",
    "\n",
    "image_size = 28\n",
    "def reformat(dataset):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  return dataset\n",
    "\n",
    "train_X = reformat(train_dataset)\n",
    "test_X = reformat(test_dataset)\n",
    "\n",
    "# initialize a logistic regression\n",
    "# l2 penalty, lbfgs algorithm, multinomial\n",
    "LR = LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0, \n",
    "                   fit_intercept=True, intercept_scaling=1, class_weight=None, \n",
    "                   random_state=None, solver='lbfgs', max_iter=100, \n",
    "                   multi_class='multinomial', verbose=0, warm_start=False, n_jobs=1)\n",
    "\n",
    "num_all_data = train_X.shape[0]\n",
    "# num_all_data = 100\n",
    "various_num_training_data = np.array([50, 100, 1000, 5000, num_all_data]);\n",
    "\n",
    "def various_training_samples(model, various_num_training_data, \n",
    "                             train_data, train_labels, test_data, test_labels):\n",
    "    for num_data in various_num_training_data:\n",
    "        model.fit(train_data[:num_data,:], train_labels[:num_data])\n",
    "        print('Number of training data:', num_data, \n",
    "              'accuracy:', model.score(test_data, test_labels))\n",
    "\n",
    "various_training_samples(LR, various_num_training_data, \n",
    "                             train_X, train_labels, test_X, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
