{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Process data\n",
    "\n",
    "The objective is to learn about simple data curation practices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This notebook uses the [notMNIST](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html) dataset to be used with python experiments. This dataset is designed to look like the classic [MNIST](http://yann.lecun.com/exdb/mnist/) dataset, while looking a little more like real data: it's a harder task, and the data is a lot less 'clean' than MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "from IPython.display import display, Image\n",
    "from scipy import ndimage\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "# Config the matplotlib backend as plotting inline in IPython\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the data to the local machine\n",
    "- The data consists of characters rendered in a variety of fonts on a 28x28 image. \n",
    "- The labels are limited to 'A' through 'J' (10 classes). \n",
    "- The training set has about 500k and the testset 19000 labeled examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to download: notMNIST_large.tar.gz\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/notMNIST_large.tar.gz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-fe85b3a4446a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdest_filename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mtrain_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'notMNIST_large.tar.gz'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m247336696\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0mtest_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'notMNIST_small.tar.gz'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8458043\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-fe85b3a4446a>\u001b[0m in \u001b[0;36mmaybe_download\u001b[0;34m(filename, expected_bytes, force)\u001b[0m\n\u001b[1;32m     25\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mforce\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdest_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Attempting to download:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreporthook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_progress_hook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nDownload Complete!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m   \u001b[0mstatinfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdest_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0;31m# Handle temporary file setup.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m             \u001b[0mtfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m             \u001b[0mtfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtempfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNamedTemporaryFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/notMNIST_large.tar.gz'"
     ]
    }
   ],
   "source": [
    "url = 'https://commondatastorage.googleapis.com/books1000/'\n",
    "last_percent_reported = None\n",
    "data_root = './data/' # Change me to store data elsewhere\n",
    "\n",
    "def download_progress_hook(count, blockSize, totalSize):\n",
    "  \"\"\"A hook to report the progress of a download. This is mostly intended for users with\n",
    "  slow internet connections. Reports every 5% change in download progress.\n",
    "  \"\"\"\n",
    "  global last_percent_reported\n",
    "  percent = int(count * blockSize * 100 / totalSize)\n",
    "\n",
    "  if last_percent_reported != percent:\n",
    "    if percent % 5 == 0:\n",
    "      sys.stdout.write(\"%s%%\" % percent)\n",
    "      sys.stdout.flush()\n",
    "    else:\n",
    "      sys.stdout.write(\".\")\n",
    "      sys.stdout.flush()\n",
    "      \n",
    "    last_percent_reported = percent\n",
    "        \n",
    "def maybe_download(filename, expected_bytes, force=False):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  dest_filename = os.path.join(data_root, filename)\n",
    "  if force or not os.path.exists(dest_filename):\n",
    "    print('Attempting to download:', filename) \n",
    "    filename, _ = urlretrieve(url + filename, dest_filename, reporthook=download_progress_hook)\n",
    "    print('\\nDownload Complete!')\n",
    "  statinfo = os.stat(dest_filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified', dest_filename)\n",
    "  else:\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + dest_filename + '. Can you get to it with a browser?')\n",
    "  return dest_filename\n",
    "\n",
    "train_filename = maybe_download('notMNIST_large.tar.gz', 247336696)\n",
    "test_filename = maybe_download('notMNIST_small.tar.gz', 8458043)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the dataset \n",
    "Extract the dataset from the compressed .tar.gz file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data for ./data/notMNIST_large. This may take a while. Please wait.\n",
      "['./data/notMNIST_large/A', './data/notMNIST_large/B', './data/notMNIST_large/C', './data/notMNIST_large/D', './data/notMNIST_large/E', './data/notMNIST_large/F', './data/notMNIST_large/G', './data/notMNIST_large/H', './data/notMNIST_large/I', './data/notMNIST_large/J']\n",
      "Extracting data for ./data/notMNIST_small. This may take a while. Please wait.\n",
      "['./data/notMNIST_small/A', './data/notMNIST_small/B', './data/notMNIST_small/C', './data/notMNIST_small/D', './data/notMNIST_small/E', './data/notMNIST_small/F', './data/notMNIST_small/G', './data/notMNIST_small/H', './data/notMNIST_small/I', './data/notMNIST_small/J']\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "np.random.seed(133)\n",
    "\n",
    "def maybe_extract(filename, force=False):\n",
    "  root = os.path.splitext(os.path.splitext(filename)[0])[0]  # remove .tar.gz\n",
    "  if os.path.isdir(root) and not force:\n",
    "    # You may override by setting force=True.\n",
    "    print('%s already present - Skipping extraction of %s.' % (root, filename))\n",
    "  else:\n",
    "    print('Extracting data for %s. This may take a while. Please wait.' % root)\n",
    "    tar = tarfile.open(filename)\n",
    "    sys.stdout.flush()\n",
    "    tar.extractall(data_root)\n",
    "    tar.close()\n",
    "  data_folders = [\n",
    "    os.path.join(root, d) for d in sorted(os.listdir(root))\n",
    "    if os.path.isdir(os.path.join(root, d))]\n",
    "  if len(data_folders) != num_classes:\n",
    "    raise Exception(\n",
    "      'Expected %d folders, one per class. Found %d instead.' % (\n",
    "        num_classes, len(data_folders)))\n",
    "  print(data_folders)\n",
    "  return data_folders\n",
    "  \n",
    "train_folders = maybe_extract(train_filename)\n",
    "test_folders = maybe_extract(test_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display a sample of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABZ0lEQVR4nMWSPWuUQRSFn7u7UUHWZAV7wSqoYGVtYaMEbWxsBBGx8w9YqAj+AS3sREhtaxUNWlgIC4looRaSJiBidoWI2fed+1i82Q9ksfV0M885c5mZE0DYXj5/7uSxIwH48/vH9bUPdQhAh1NPh6YTpcPVM3QCIlq3fljqMqFZ6trB7Q4B7ftWddGcSLOufNgBblgVtZ4eW6s58iZxfLNrQLaqwR4AB5cWsgWwe5q7FjVdu9ADgN7FV6ZavEdftfiyC0REAN11i2qfgerIFTrRJKPDiiPVndYigGyTza0x2UaApRb7fmY0Xoyhs9C/4Fz9Txj/gs6FwyZ3dC782uSvH8oYa/pCLxDaXn10YvLZkxmx/J42AIPdZi+/PPv0dnSgwXesUrNMa/Jro+liwuFV92pnC1YaZ0qw+KTOmWaaxSxa3IKI9pV3lTMavflsml4DokXv8uP+t33D740HC2efb+28vsQfU/ItPYCuJtMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "image/png": {
       "height": 200,
       "width": 200
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(filename='./data/notMNIST_large/A/a2F6b28udHRm.png', width=200, height=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert into arrays\n",
    "- Convert the entire dataset into a 3D array (image index, x, y)(**number of data x 28 x 28**) of floating point values.\n",
    "- Normalize the data to have approximately zero mean and standard deviation ~0.5.\n",
    "- Skip images that are not readable.\n",
    "- Load each class into a separate dataset, since the computer might not be able to fit all data in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling ./data/notMNIST_large/A.pickle.\n",
      "./data/notMNIST_large/A\n",
      "Could not read: ./data/notMNIST_large/A/RnJlaWdodERpc3BCb29rSXRhbGljLnR0Zg==.png : cannot identify image file './data/notMNIST_large/A/RnJlaWdodERpc3BCb29rSXRhbGljLnR0Zg==.png' - it's ok, skipping.\n",
      "Could not read: ./data/notMNIST_large/A/SG90IE11c3RhcmQgQlROIFBvc3Rlci50dGY=.png : cannot identify image file './data/notMNIST_large/A/SG90IE11c3RhcmQgQlROIFBvc3Rlci50dGY=.png' - it's ok, skipping.\n",
      "Could not read: ./data/notMNIST_large/A/Um9tYW5hIEJvbGQucGZi.png : cannot identify image file './data/notMNIST_large/A/Um9tYW5hIEJvbGQucGZi.png' - it's ok, skipping.\n",
      "Full dataset tensor: (52909, 28, 28)\n",
      "Mean: -0.12825\n",
      "Standard deviation: 0.443121\n",
      "Pickling ./data/notMNIST_large/B.pickle.\n",
      "./data/notMNIST_large/B\n",
      "Could not read: ./data/notMNIST_large/B/TmlraXNFRi1TZW1pQm9sZEl0YWxpYy5vdGY=.png : cannot identify image file './data/notMNIST_large/B/TmlraXNFRi1TZW1pQm9sZEl0YWxpYy5vdGY=.png' - it's ok, skipping.\n",
      "Full dataset tensor: (52911, 28, 28)\n",
      "Mean: -0.00756303\n",
      "Standard deviation: 0.454491\n",
      "Pickling ./data/notMNIST_large/C.pickle.\n",
      "./data/notMNIST_large/C\n",
      "Full dataset tensor: (52912, 28, 28)\n",
      "Mean: -0.142258\n",
      "Standard deviation: 0.439806\n",
      "Pickling ./data/notMNIST_large/D.pickle.\n",
      "./data/notMNIST_large/D\n",
      "Could not read: ./data/notMNIST_large/D/VHJhbnNpdCBCb2xkLnR0Zg==.png : cannot identify image file './data/notMNIST_large/D/VHJhbnNpdCBCb2xkLnR0Zg==.png' - it's ok, skipping.\n",
      "Full dataset tensor: (52911, 28, 28)\n",
      "Mean: -0.0573678\n",
      "Standard deviation: 0.455648\n",
      "Pickling ./data/notMNIST_large/E.pickle.\n",
      "./data/notMNIST_large/E\n",
      "Full dataset tensor: (52912, 28, 28)\n",
      "Mean: -0.069899\n",
      "Standard deviation: 0.452942\n",
      "Pickling ./data/notMNIST_large/F.pickle.\n",
      "./data/notMNIST_large/F\n",
      "Full dataset tensor: (52912, 28, 28)\n",
      "Mean: -0.125583\n",
      "Standard deviation: 0.44709\n",
      "Pickling ./data/notMNIST_large/G.pickle.\n",
      "./data/notMNIST_large/G\n",
      "Full dataset tensor: (52912, 28, 28)\n",
      "Mean: -0.0945814\n",
      "Standard deviation: 0.44624\n",
      "Pickling ./data/notMNIST_large/H.pickle.\n",
      "./data/notMNIST_large/H\n",
      "Full dataset tensor: (52912, 28, 28)\n",
      "Mean: -0.0685221\n",
      "Standard deviation: 0.454232\n",
      "Pickling ./data/notMNIST_large/I.pickle.\n",
      "./data/notMNIST_large/I\n",
      "Full dataset tensor: (52912, 28, 28)\n",
      "Mean: 0.0307862\n",
      "Standard deviation: 0.468899\n",
      "Pickling ./data/notMNIST_large/J.pickle.\n",
      "./data/notMNIST_large/J\n",
      "Full dataset tensor: (52911, 28, 28)\n",
      "Mean: -0.153358\n",
      "Standard deviation: 0.443656\n",
      "Pickling ./data/notMNIST_small/A.pickle.\n",
      "./data/notMNIST_small/A\n",
      "Could not read: ./data/notMNIST_small/A/RGVtb2NyYXRpY2FCb2xkT2xkc3R5bGUgQm9sZC50dGY=.png : cannot identify image file './data/notMNIST_small/A/RGVtb2NyYXRpY2FCb2xkT2xkc3R5bGUgQm9sZC50dGY=.png' - it's ok, skipping.\n",
      "Full dataset tensor: (1872, 28, 28)\n",
      "Mean: -0.132626\n",
      "Standard deviation: 0.445128\n",
      "Pickling ./data/notMNIST_small/B.pickle.\n",
      "./data/notMNIST_small/B\n",
      "Full dataset tensor: (1873, 28, 28)\n",
      "Mean: 0.00535609\n",
      "Standard deviation: 0.457115\n",
      "Pickling ./data/notMNIST_small/C.pickle.\n",
      "./data/notMNIST_small/C\n",
      "Full dataset tensor: (1873, 28, 28)\n",
      "Mean: -0.141521\n",
      "Standard deviation: 0.44269\n",
      "Pickling ./data/notMNIST_small/D.pickle.\n",
      "./data/notMNIST_small/D\n",
      "Full dataset tensor: (1873, 28, 28)\n",
      "Mean: -0.0492167\n",
      "Standard deviation: 0.459759\n",
      "Pickling ./data/notMNIST_small/E.pickle.\n",
      "./data/notMNIST_small/E\n",
      "Full dataset tensor: (1873, 28, 28)\n",
      "Mean: -0.0599148\n",
      "Standard deviation: 0.45735\n",
      "Pickling ./data/notMNIST_small/F.pickle.\n",
      "./data/notMNIST_small/F\n",
      "Could not read: ./data/notMNIST_small/F/Q3Jvc3NvdmVyIEJvbGRPYmxpcXVlLnR0Zg==.png : cannot identify image file './data/notMNIST_small/F/Q3Jvc3NvdmVyIEJvbGRPYmxpcXVlLnR0Zg==.png' - it's ok, skipping.\n",
      "Full dataset tensor: (1872, 28, 28)\n",
      "Mean: -0.118185\n",
      "Standard deviation: 0.452279\n",
      "Pickling ./data/notMNIST_small/G.pickle.\n",
      "./data/notMNIST_small/G\n",
      "Full dataset tensor: (1872, 28, 28)\n",
      "Mean: -0.0925503\n",
      "Standard deviation: 0.449006\n",
      "Pickling ./data/notMNIST_small/H.pickle.\n",
      "./data/notMNIST_small/H\n",
      "Full dataset tensor: (1872, 28, 28)\n",
      "Mean: -0.0586893\n",
      "Standard deviation: 0.458759\n",
      "Pickling ./data/notMNIST_small/I.pickle.\n",
      "./data/notMNIST_small/I\n",
      "Full dataset tensor: (1872, 28, 28)\n",
      "Mean: 0.0526451\n",
      "Standard deviation: 0.471894\n",
      "Pickling ./data/notMNIST_small/J.pickle.\n",
      "./data/notMNIST_small/J\n",
      "Full dataset tensor: (1872, 28, 28)\n",
      "Mean: -0.151689\n",
      "Standard deviation: 0.448014\n"
     ]
    }
   ],
   "source": [
    "image_size = 28  # Pixel width and height.\n",
    "pixel_depth = 255.0  # Number of levels per pixel.\n",
    "\n",
    "def load_letter(folder, min_num_images):\n",
    "  \"\"\"Load the data for a single letter label.\"\"\"\n",
    "  image_files = os.listdir(folder)\n",
    "  dataset = np.ndarray(shape=(len(image_files), image_size, image_size),\n",
    "                         dtype=np.float32)\n",
    "  print(folder)\n",
    "  num_images = 0\n",
    "  for image in image_files:\n",
    "    image_file = os.path.join(folder, image)\n",
    "    try:\n",
    "      image_data = (ndimage.imread(image_file).astype(float) - \n",
    "                    pixel_depth / 2) / pixel_depth\n",
    "      if image_data.shape != (image_size, image_size):\n",
    "        raise Exception('Unexpected image shape: %s' % str(image_data.shape))\n",
    "      dataset[num_images, :, :] = image_data\n",
    "      num_images = num_images + 1\n",
    "    except IOError as e:\n",
    "      print('Could not read:', image_file, ':', e, '- it\\'s ok, skipping.')\n",
    "    \n",
    "  dataset = dataset[0:num_images, :, :]\n",
    "  if num_images < min_num_images:\n",
    "    raise Exception('Many fewer images than expected: %d < %d' %\n",
    "                    (num_images, min_num_images))\n",
    "    \n",
    "  print('Full dataset tensor:', dataset.shape)\n",
    "  print('Mean:', np.mean(dataset))\n",
    "  print('Standard deviation:', np.std(dataset))\n",
    "  return dataset\n",
    "        \n",
    "def maybe_pickle(data_folders, min_num_images_per_class, force=False):\n",
    "  dataset_names = []\n",
    "  for folder in data_folders:\n",
    "    set_filename = folder + '.pickle'\n",
    "    dataset_names.append(set_filename)\n",
    "    if os.path.exists(set_filename) and not force:\n",
    "      # You may override by setting force=True.\n",
    "      print('%s already present - Skipping pickling.' % set_filename)\n",
    "    else:\n",
    "      print('Pickling %s.' % set_filename)\n",
    "      dataset = load_letter(folder, min_num_images_per_class)\n",
    "      try:\n",
    "        with open(set_filename, 'wb') as f:\n",
    "          pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)\n",
    "      except Exception as e:\n",
    "        print('Unable to save data to', set_filename, ':', e)\n",
    "  \n",
    "  return dataset_names\n",
    "\n",
    "train_datasets = maybe_pickle(train_folders, 45000)\n",
    "test_datasets = maybe_pickle(test_folders, 1800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displaying a sample from an array\n",
    "Use [matplotlib.pyplot](https://matplotlib.org/users/image_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-64c644435842>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mclass_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msample_class_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_datasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclass_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msample_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdisplay_a_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_class_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msample_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_datasets' is not defined"
     ]
    }
   ],
   "source": [
    "class_index = 0;\n",
    "sample_class_path = train_datasets[class_index];\n",
    "sample_index = 1;\n",
    "\n",
    "def display_a_sample(sample_class_path,sample_index):\n",
    "    print(\"The label of the class:\", sample_class_path)\n",
    "    try:\n",
    "        with open(sample_class_path, 'rb') as f:\n",
    "            letter_set = pickle.load(f)\n",
    "            a_letter = letter_set[sample_index,:,:];\n",
    "            imgplot = plt.imshow(a_letter)\n",
    "    except Exception as e:\n",
    "        print('Unable to process data from', sample_class_path, ':', e)\n",
    "\n",
    "display_a_sample(sample_class_path,sample_index)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify that the data is balanced\n",
    "We expect the data to be balanced across classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of training data across classes: [ 52909.  52911.  52912.  52911.  52912.  52912.  52912.  52912.  52912.\n",
      "  52911.]\n",
      "Distribution of test data across classes: [ 1872.  1873.  1873.  1873.  1873.  1872.  1872.  1872.  1872.  1872.]\n"
     ]
    }
   ],
   "source": [
    "# train_datasets, test_datasets\n",
    "def verify_balance(pickle_files):\n",
    "  num_classes = len(pickle_files)\n",
    "  num_data_per_class = np.zeros(num_classes);\n",
    "\n",
    "  for i, pickle_file in enumerate(pickle_files):       \n",
    "    try:\n",
    "      with open(pickle_file, 'rb') as f:\n",
    "        letter_set = pickle.load(f)\n",
    "        num_data_per_class[i] = letter_set.shape[0]        \n",
    "    except Exception as e:\n",
    "      print('Unable to process data from', pickle_file, ':', e)\n",
    "    \n",
    "  return num_data_per_class\n",
    "\n",
    "print(\"Distribution of training data across classes:\", verify_balance(train_datasets))\n",
    "print(\"Distribution of test data across classes:\", verify_balance(test_datasets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge and prune data, and create a validation datasets\n",
    "Merge and prune the training data as needed. The labels will be stored into a separate array of integers 0 through 9. Also create a validation dataset for hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: (200000, 28, 28) (200000,)\n",
      "Validation: (10000, 28, 28) (10000,)\n",
      "Testing: (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "def make_arrays(nb_rows, img_size):\n",
    "  if nb_rows:\n",
    "    dataset = np.ndarray((nb_rows, img_size, img_size), dtype=np.float32)\n",
    "    labels = np.ndarray(nb_rows, dtype=np.int32)\n",
    "  else:\n",
    "    dataset, labels = None, None\n",
    "  return dataset, labels\n",
    "\n",
    "def merge_datasets(pickle_files, train_size, valid_size=0):\n",
    "  num_classes = len(pickle_files)\n",
    "  valid_dataset, valid_labels = make_arrays(valid_size, image_size)\n",
    "  train_dataset, train_labels = make_arrays(train_size, image_size)\n",
    "  vsize_per_class = valid_size // num_classes\n",
    "  tsize_per_class = train_size // num_classes\n",
    "    \n",
    "  start_v, start_t = 0, 0\n",
    "  end_v, end_t = vsize_per_class, tsize_per_class\n",
    "  end_l = vsize_per_class+tsize_per_class\n",
    "  for label, pickle_file in enumerate(pickle_files):       \n",
    "    try:\n",
    "      with open(pickle_file, 'rb') as f:\n",
    "        letter_set = pickle.load(f)\n",
    "        # let's shuffle the letters to have random validation and training set\n",
    "        np.random.shuffle(letter_set)\n",
    "        if valid_dataset is not None:\n",
    "          valid_letter = letter_set[:vsize_per_class, :, :]\n",
    "          valid_dataset[start_v:end_v, :, :] = valid_letter\n",
    "          valid_labels[start_v:end_v] = label\n",
    "          start_v += vsize_per_class\n",
    "          end_v += vsize_per_class\n",
    "                    \n",
    "        train_letter = letter_set[vsize_per_class:end_l, :, :]\n",
    "        train_dataset[start_t:end_t, :, :] = train_letter\n",
    "        train_labels[start_t:end_t] = label\n",
    "        start_t += tsize_per_class\n",
    "        end_t += tsize_per_class\n",
    "    except Exception as e:\n",
    "      print('Unable to process data from', pickle_file, ':', e)\n",
    "      raise\n",
    "    \n",
    "  return valid_dataset, valid_labels, train_dataset, train_labels\n",
    "            \n",
    "            \n",
    "train_size = 200000\n",
    "valid_size = 10000\n",
    "test_size = 10000\n",
    "\n",
    "valid_dataset, valid_labels, train_dataset, train_labels = merge_datasets(\n",
    "  train_datasets, train_size, valid_size)\n",
    "_, _, test_dataset, test_labels = merge_datasets(test_datasets, test_size)\n",
    "\n",
    "print('Training:', train_dataset.shape, train_labels.shape)\n",
    "print('Validation:', valid_dataset.shape, valid_labels.shape)\n",
    "print('Testing:', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle the data\n",
    "It's important to have data well shuffled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def randomize(dataset, labels):\n",
    "  permutation = np.random.permutation(labels.shape[0])\n",
    "  shuffled_dataset = dataset[permutation,:,:]\n",
    "  shuffled_labels = labels[permutation]\n",
    "  return shuffled_dataset, shuffled_labels\n",
    "train_dataset, train_labels = randomize(train_dataset, train_labels)\n",
    "test_dataset, test_labels = randomize(test_dataset, test_labels)\n",
    "valid_dataset, valid_labels = randomize(valid_dataset, valid_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify the shuffled data\n",
    "Verify that the data is still good after shuffling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nivADmSL8QKb+H4TILe6AV9r7AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10ba7e0b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_index = 1;\n",
    "a_sample = train_dataset[sample_index,:,:];\n",
    "imgplot = plt.imshow(a_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the data\n",
    "Save the data for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressed pickle size: 690800506\n"
     ]
    }
   ],
   "source": [
    "pickle_file = os.path.join(data_root, 'notMNIST.pickle')\n",
    "\n",
    "try:\n",
    "  f = open(pickle_file, 'wb')\n",
    "  save = {\n",
    "    'train_dataset': train_dataset,\n",
    "    'train_labels': train_labels,\n",
    "    'valid_dataset': valid_dataset,\n",
    "    'valid_labels': valid_labels,\n",
    "    'test_dataset': test_dataset,\n",
    "    'test_labels': test_labels,\n",
    "    }\n",
    "  pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "  f.close()\n",
    "except Exception as e:\n",
    "  print('Unable to save data to', pickle_file, ':', e)\n",
    "  raise\n",
    "    \n",
    "statinfo = os.stat(pickle_file)\n",
    "print('Compressed pickle size:', statinfo.st_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train an logistic regression\n",
    "Train a logistic regression model on this data using 50, 100, 1000 and 5000 training samples using [sklearn.linear_model.LogistricRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression).\n",
    "### Reference\n",
    "[Notes of logistic regression](https://github.com/xiangli-chen/home/blob/master/machine-learning/general-supervised-learning.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Test set (10000, 28, 28) (10000,)\n",
      "Number of training data: 50 accuracy: 0.4849\n",
      "Number of training data: 100 accuracy: 0.7136\n",
      "Number of training data: 1000 accuracy: 0.8425\n",
      "Number of training data: 5000 accuracy: 0.8438\n",
      "Number of training data: 200000 accuracy: 0.8999\n"
     ]
    }
   ],
   "source": [
    "# reformulate the data\n",
    "# data: {array-like, sparse matrix}, shape (n_samples, n_features)\n",
    "# label: rray-like, shape (n_samples,)\n",
    "\n",
    "pickle_file = './data/notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)\n",
    "\n",
    "image_size = 28\n",
    "def reformat(dataset):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  return dataset\n",
    "\n",
    "train_X = reformat(train_dataset)\n",
    "test_X = reformat(test_dataset)\n",
    "\n",
    "# initialize a logistic regression\n",
    "# l2 penalty, lbfgs algorithm, multinomial\n",
    "LR = LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0, \n",
    "                   fit_intercept=True, intercept_scaling=1, class_weight=None, \n",
    "                   random_state=None, solver='lbfgs', max_iter=100, \n",
    "                   multi_class='multinomial', verbose=0, warm_start=False, n_jobs=1)\n",
    "\n",
    "num_all_data = train_X.shape[0]\n",
    "# num_all_data = 100\n",
    "various_num_training_data = np.array([50, 100, 1000, 5000, num_all_data]);\n",
    "\n",
    "def various_training_samples(model, various_num_training_data, \n",
    "                             train_data, train_labels, test_data, test_labels):\n",
    "    for num_data in various_num_training_data:\n",
    "        model.fit(train_data[:num_data,:], train_labels[:num_data])\n",
    "        print('Number of training data:', num_data, \n",
    "              'accuracy:', model.score(test_data, test_labels))\n",
    "\n",
    "various_training_samples(LR, various_num_training_data, \n",
    "                             train_X, train_labels, test_X, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
